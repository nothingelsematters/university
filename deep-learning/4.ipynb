{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from lets_plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# from datalore.display import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def to_scalar(x, i):\n",
    "    \"\"\"Returns i-th element from an array x if x is array, else returns x\n",
    "Args:\n",
    "    x - input array\n",
    "    i - index to return\n",
    "Returns:\n",
    "    x[i] if x is an array and x else\n",
    "\"\"\"\n",
    "    if isinstance(x, (np.ndarray, list)):\n",
    "        return x[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checker(J, grad_J, theta, eps=1e-4, rtol=1e-5):\n",
    "    \"\"\"Gradient checker for scalar and vector functions\n",
    "Args:\n",
    "    J - function of theta\n",
    "    grad_J - gradient of function J\n",
    "    theta - the point for which to compute the numerical gradient\n",
    "    eps - step value in numerical gradient\n",
    "    rtol - relative tolerance threshold value\n",
    "Returns:\n",
    "    error message if the relative tolerance is greater for some axis\n",
    "    or \"Gradient check passed\" else\n",
    "\"\"\"\n",
    "    it = np.nditer(theta, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] += eps\n",
    "        np.random.seed(42)\n",
    "        J1 = J(theta_)\n",
    "\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] -= eps\n",
    "        np.random.seed(42)\n",
    "        J2 = J(theta_)\n",
    "\n",
    "        J1 = to_scalar(J1, ix)\n",
    "        J2 = to_scalar(J2, ix)\n",
    "\n",
    "        num_grad = (J1 - J2)/(2*eps)\n",
    "\n",
    "        rel_tol = np.abs(num_grad - grad_J(theta))[ix]/(1. + np.minimum(np.abs(num_grad), np.abs(grad_J(theta)[ix])))\n",
    "\n",
    "        if np.all(rel_tol > rtol):\n",
    "            print(f'Incorrect gradient for the axis {str(ix)}')\n",
    "            return\n",
    "        it.iternext()\n",
    "    print(f'Gradient check passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def J_theta_global(model, loss_function, theta, idx, x, y):\n",
    "    previous = model.parameters()[idx].copy()\n",
    "    np.copyto(dst=model.parameters()[idx], src=theta)\n",
    "    outputs = model(x)\n",
    "    loss = loss_function(outputs, y)\n",
    "    np.copyto(dst=model.parameters()[idx], src=previous)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def dJ_theta_global(model, loss_function, theta, idx, x):\n",
    "    grad = model.backward(loss_function)[idx] / x.shape[0]\n",
    "    return grad.reshape(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,slope=1.0):\n",
    "    return 1.0/(1.0+np.exp(-slope*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(x,slope=1.0):\n",
    "    return slope*sigmoid(x,slope=slope)*(1.0-sigmoid(x,slope=slope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed\n"
     ]
    }
   ],
   "source": [
    "z = np.random.normal(size=5)\n",
    "gradient_checker(sigmoid, sigmoid_prime, z, eps=1e-4, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def plot_digit(digit, size=8, caption=None):\n",
    "    digit = digit.reshape(size, size)\n",
    "    digit = (digit - np.min(digit))/(np.max(digit) - np.min(digit))\n",
    "    p = ggplot() + geom_image(image_data=digit) + labs(x='', y='') \\\n",
    "        + theme(axis_line='blank', axis_title='blank', axis_ticks='blank', axis_text='blank')\n",
    "    if caption:\n",
    "        p += ggtitle(caption)\n",
    "    return p;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def gg_confusion_matrix(y, y_hat):\n",
    "    conf_mat = confusion_matrix(y, y_hat)[::-1]\n",
    "    confusion_dat = pd.DataFrame(conf_mat)\n",
    "    observed = confusion_dat.columns.values\n",
    "    actual = confusion_dat.index.values\n",
    "    xx, yy = np.meshgrid(actual, observed)\n",
    "    xx = xx.reshape(-1)\n",
    "    yy = yy.reshape(-1)\n",
    "    zz = conf_mat.reshape(-1)\n",
    "    dat = {'predicted':xx, 'actual':yy[::-1], 'z':zz}\n",
    "    p = ggplot(dat, aes('predicted', 'actual', fill='z')) \\\n",
    "        + geom_raster() \\\n",
    "        + geom_text(aes(label='z'), color='white')\\\n",
    "        + theme(legend_position='none', axis_ticks='blank', axis_line='blank')\\\n",
    "        + ggsize(500, 500) + scale_x_discrete() + scale_y_discrete()\\\n",
    "        + ggtitle('Confusion matrix')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score_micro(conf_matrix):\n",
    "    num_tags = conf_matrix.shape[0]\n",
    "    score = 0.\n",
    "    pr, p, r = 0., 0., 0.\n",
    "    for tag in range(num_tags):\n",
    "        pr += conf_matrix[tag, tag]\n",
    "        p += sum(conf_matrix[tag, :])\n",
    "        r += sum(conf_matrix[:, tag])\n",
    "    try:\n",
    "        score = 2 * pr / (p + r)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, grads, params, learning_rate=None):\n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.learning_rate\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= learning_rate*grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, labels, title='Confusion matrix', cmap='Oranges'):\n",
    "    norm_cm = conf_matrix / conf_matrix.sum(axis=0)\n",
    "    norm_cm[norm_cm != norm_cm] = .0  # eliminate NaN\n",
    "\n",
    "    fig = Figure(figsize=(7, 7), dpi=120, facecolor='w', edgecolor='k')\n",
    "    ax: Axes = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(label=title)\n",
    "\n",
    "    ax.imshow(norm_cm, cmap=cmap)\n",
    "    tick_marks = np.arange(len(labels))\n",
    "\n",
    "    ax.set_xlabel('Actual', fontsize=7)\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(labels, fontsize=6, rotation=-90, ha='center')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.set_ylabel('Predicted', fontsize=7)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(labels, fontsize=6, va='center')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    ax.yaxis.tick_left()\n",
    "\n",
    "    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "        ax.text(j, i, format(int(conf_matrix[i, j]), 'd') if conf_matrix[i, j] != 0 else '.',\n",
    "        horizontalalignment='center', verticalalignment='center', fontsize=6, color='black')\n",
    "    fig.set_tight_layout('true')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, labels, title='Confusion matrix', cmap: str='Oranges'):\n",
    "    norm_cm = conf_matrix / conf_matrix.sum(axis=0)\n",
    "    norm_cm[norm_cm != norm_cm] = .0  # eliminate NaN\n",
    "    \n",
    "    fig: Figure = Figure(figsize=(7, 7))\n",
    "    ax: Axes = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(label=title)\n",
    "\n",
    "    ax.matshow(norm_cm, cmap=cmap)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "\n",
    "    ax.set_xticklabels([''] + labels, rotation=90)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "        ax.text(j, i, format(int(conf_matrix[i, j]), 'd') if conf_matrix[i, j] != 0 else '.',\n",
    "                horizontalalignment='center', verticalalignment='center', fontsize=6, color='black')\n",
    "    fig.set_tight_layout('true')\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import inspect\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union, Callable\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "seq_len = 3\n",
    "max_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def get_examples(seq_len, num_examples, max_number):\n",
    "    inputs = np.random.randint(0, max_number, size=(num_examples, seq_len))\n",
    "    targets = np.sort(inputs)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "inputs, targets = get_examples(seq_len, num_examples, max_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs/sorted: ([6, 3, 7], [3, 6, 7])\n",
      "inputs/sorted: ([4, 6, 9], [4, 6, 9])\n",
      "inputs/sorted: ([2, 6, 7], [2, 6, 7])\n",
      "inputs/sorted: ([4, 3, 7], [3, 4, 7])\n",
      "inputs/sorted: ([7, 2, 5], [2, 5, 7])\n",
      "inputs/sorted: ([4, 1, 7], [1, 4, 7])\n",
      "inputs/sorted: ([5, 1, 4], [1, 4, 5])\n",
      "inputs/sorted: ([0, 9, 5], [0, 5, 9])\n",
      "inputs/sorted: ([8, 0, 9], [0, 8, 9])\n",
      "inputs/sorted: ([2, 6, 3], [2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "for inp, tgt in zip(inputs[:10], targets[:10]):\n",
    "    print(f'inputs/sorted: {list(inp), list(tgt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def to_string(X, seq_len, max_number):\n",
    "    max_length = int(seq_len * np.ceil(np.log10(max_number + 1)) + seq_len - 1)\n",
    "    Xstr = []\n",
    "    for example in X:\n",
    "        xstr = ','.join([str(n) for n in example])\n",
    "        xstr += ''.join([' ' for _ in range(max_length - len(xstr))])\n",
    "        Xstr.append(xstr)\n",
    "    return Xstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "inputs = to_string(inputs, seq_len, max_number)\n",
    "targets = to_string(targets, seq_len, max_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: 6,3,7    targets[0]: 3,6,7   \n"
     ]
    }
   ],
   "source": [
    "print(f'inputs[0]: {inputs[0]} targets[0]: {targets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def integer_encode(X, vocab):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "    Xenc = []\n",
    "    for example in X:\n",
    "        encoded = [char_to_int[char] for char in example]\n",
    "        Xenc.append(encoded)\n",
    "    return Xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "inputs = integer_encode(inputs, vocab)\n",
    "targets = integer_encode(targets, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: [6, 10, 3, 10, 7, 11, 11, 11] targets[0]: [3, 10, 6, 10, 7, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "print(f'inputs[0]: {inputs[0]} targets[0]: {targets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def count_forward_calls(obj):\n",
    "    for prop, value in vars(obj).items():\n",
    "        if isinstance(value, Tensor):\n",
    "            value._forward_calls += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def forks_aware(FunctionClass):\n",
    "    class WrappedClass:\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            self.FunctionClass = FunctionClass(*args, **kwargs)\n",
    "        \n",
    "        def __call__(self):\n",
    "            result = self.FunctionClass()\n",
    "            count_forward_calls(self.FunctionClass)\n",
    "            return result\n",
    "    return WrappedClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class Function:\n",
    "    def __call__(self) -> \"Tensor\":\n",
    "        pass\n",
    "    \n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data: np.ndarray, func: Optional[Function]=None, name: str=None):\n",
    "        self.data: np.ndarray = data\n",
    "        self.grad: np.ndarray = np.zeros(data.shape)\n",
    "        self.func = func\n",
    "        self.__name__ = name\n",
    "        self._forward_calls = 0\n",
    "        self._backward_calls = 0\n",
    "    \n",
    "    def backward(self, grad: Optional[np.ndarray] = None):\n",
    "        self._backward_calls += 1\n",
    "        # print(f'{self.__name__} backward: {self._backward_calls} forward: {self._forward_calls}')\n",
    "        if grad is not None:\n",
    "            assert grad.shape == self.grad.shape\n",
    "            self.grad += grad\n",
    "            if self.func: # and self._forward_calls <= self._backward_calls:\n",
    "                self.func.backward(grad)\n",
    "        else:\n",
    "            if self.func:\n",
    "                self.func.backward()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad[:] = .0\n",
    "        self._forward_calls = 0\n",
    "        self._backward_calls = 0\n",
    "    \n",
    "    def reshape(self, *args, **kwargs):\n",
    "        return Tensor(self.data.reshape(*args, **kwargs), self.func, self.__name__)\n",
    "    \n",
    "    def transpose(self, *args, **kwargs):\n",
    "        return Tensor(self.data.transpose(*args, **kwargs), self.func, self.__name__)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.data.size\n",
    "    \n",
    "    def astype(self, dtype: Union[str, np.dtype]):\n",
    "        return self.data.astype(dtype)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.parameters: List[Tensor] = []\n",
    "        self.__name__ = self.__class__.__name__\n",
    "        self.state_dict = {}\n",
    "        self.training = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_module_state_dict(module: \"Module\"):\n",
    "        keys = [param.__name__ for param in module.__dict__['parameters']]\n",
    "        values = [param.data.tolist() for param in module.__dict__['parameters']]\n",
    "        return dict(zip(keys, values))\n",
    "     \n",
    "    def update_state_dict(self):\n",
    "        module_state_dicts = []\n",
    "        module_names = []\n",
    "        for key in self.__dict__:\n",
    "            value = self.__dict__[key]\n",
    "            base_class_name = value.__class__.__bases__[0].__name__\n",
    "            # class_name = value.__class__.__name__\n",
    "            if base_class_name == 'Module':\n",
    "                class_has_parameters = hasattr(value, \"parameters\")\n",
    "                if class_has_parameters:\n",
    "                    parameters_not_empty = len(value.parameters) > 0\n",
    "                    if parameters_not_empty:\n",
    "                        module_names.append(key)\n",
    "                        module_state_dict = self.get_module_state_dict(value)\n",
    "                        module_state_dicts.append(module_state_dict)\n",
    "        self.state_dict = dict(zip(module_names, module_state_dicts))\n",
    "\n",
    "    def register_parameter(self, param: Tensor):\n",
    "        self.parameters.append(param)\n",
    "\n",
    "    def register_parameters(self, param_list_or_module: Union[List[Tensor], \"Module\", List[\"Module\"]]):\n",
    "        if isinstance(param_list_or_module, List):\n",
    "            for element in param_list_or_module:\n",
    "                if isinstance(element, Tensor):\n",
    "                    self.register_parameter(element)\n",
    "                elif isinstance(element, Module):\n",
    "                    for param in element.parameters:\n",
    "                        self.register_parameter(param)\n",
    "                else:\n",
    "                    raise TypeError(\"Parameter should be of type Tensor\")\n",
    "        elif isinstance(param_list_or_module, Module):\n",
    "            for param in param_list_or_module.parameters:\n",
    "                self.register_parameter(param)\n",
    "        self.update_state_dict()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            param.zero_grad()\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "    def size(self):\n",
    "        s = 0\n",
    "        for param in self.parameters:\n",
    "            s += param.data.size\n",
    "        return s\n",
    "    \n",
    "    def update_parameters_from_state_dict(self):\n",
    "        for key in self.__dict__:\n",
    "            if key in self.state_dict:\n",
    "                for param in self.__dict__[key].parameters:\n",
    "                    param.data = np.asarray(self.state_dict[key][param.__name__])\n",
    "\n",
    "    def save(self, filename: str = None):\n",
    "        if filename is None:\n",
    "            filename = time.strftime(\"%Y%m%d-%H%M%S\") + '.json'\n",
    "        self.update_state_dict()\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.state_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    def load(self, filename: str):\n",
    "        with open(filename, 'r') as f:\n",
    "            json_str = f.read()\n",
    "            self.state_dict = json.loads(json_str)\n",
    "        self.update_parameters_from_state_dict()\n",
    "    \n",
    "    def train(self):\n",
    "        for key in self.__dict__:\n",
    "            module = self.__dict__[key]\n",
    "            base_class_name = module.__class__.__bases__[0].__name__\n",
    "            if base_class_name == 'Module':\n",
    "                module.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        for key in self.__dict__:\n",
    "            module = self.__dict__[key]\n",
    "            base_class_name = module.__class__.__bases__[0].__name__\n",
    "            if base_class_name == 'Module':\n",
    "                module.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def xavier_(weights):\n",
    "    for weight in weights:\n",
    "        in_dim, out_dim = weight.shape[-2:]\n",
    "        np.copyto(dst=weight.data, src=np.random.randn(*weight.shape) * np.sqrt(2. / (in_dim + out_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def Wandb(in_dim, out_dim):\n",
    "    W = np.random.normal(loc=0, scale=0.1, size=(in_dim, out_dim))\n",
    "    b = np.random.normal(loc=0, scale=0.1, size=(1, out_dim))\n",
    "    return Tensor(W, name='weights'), Tensor(b, name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class Linear(Function):\n",
    "    def __init__(self, x: Tensor, W: Tensor, b: Tensor = None):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "    \n",
    "    def __call__(self):\n",
    "        outputs = np.dot(self.x.data, self.W.data) + self.b.data\n",
    "        return Tensor(outputs, func=self, name=\"linear\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Linear: x: {self.x.shape} W: {self.W.shape} b: {self.b.shape} grad: {grad.shape}')\n",
    "        dW = np.dot(self.x.data.T, grad)\n",
    "        db = grad.sum(axis=0)\n",
    "        grad = np.dot(grad, self.W.data.T)\n",
    "        self.W.backward(dW.reshape(self.W.shape))\n",
    "        self.b.backward(db.reshape(self.b.shape))\n",
    "        self.x.backward(grad.reshape(self.x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.W, self.b = Wandb(in_dim, out_dim)\n",
    "        self.register_parameters([self.W, self.b])\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Linear(x, self.W, self.b)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class EmbeddingFunction(Function):\n",
    "    def __init__(self, x: Tensor, E: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.E = E\n",
    "\n",
    "    def __call__(self):\n",
    "        embeddings = self.E.data[self.x.data.astype('int'), :]\n",
    "        return Tensor(embeddings, func=self, name=\"embedding\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Embedding: x: {self.x.shape} E: {self.E.shape} grad: {grad.shape}')\n",
    "        dE = np.zeros_like(self.E.data)\n",
    "        np.add.at(dE, self.x.data, grad)\n",
    "        self.E.backward(dE.reshape(self.E.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Embedding(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.E = Tensor(np.random.normal(loc=0, scale=0.1, size=(vocab_size, emb_size)), name='E')\n",
    "        self.register_parameters([self.E])\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return EmbeddingFunction(x, self.E)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1.0 / (1.0 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class Sigmoid(Function):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __call__(self):\n",
    "        self.a = sigmoid(self.x.data)\n",
    "        return Tensor(self.a, func=self, name=\"sigmoid\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Sigmoid: x: {self.x.shape} grad: {grad.shape}')\n",
    "        grad = self.a * (1. - self.a) * grad.reshape(self.a.shape)\n",
    "        self.x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class SigmoidFunction(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Sigmoid(x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class Tanh(Function):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __call__(self):\n",
    "        self.a = np.tanh(self.x.data)\n",
    "        return Tensor(self.a, func=self, name=\"tanh\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Tanh: x: {self.x.shape} grad: {grad.shape}')\n",
    "        grad = (1. - self.a ** 2) * grad.reshape(self.a.shape)\n",
    "        self.x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class TanhFunction(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Tanh(x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class GetHStack(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        super().__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        stacked = np.hstack((self.x1.data, self.x2.data))\n",
    "        return Tensor(stacked, func=self, name=\"hstack\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'HStack: x1: {self.x1.shape} x2: {self.x2.shape} grad: {grad.shape}')\n",
    "        assert grad.shape[1] == (self.x1.shape[1] + self.x2.shape[1])\n",
    "        self.x1.backward(grad[:, :self.x1.shape[1]])\n",
    "        self.x2.backward(grad[:, self.x1.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class HStack(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return GetHStack(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class GetVStack(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        super().__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        stacked = np.vstack((self.x1.data, self.x2.data))\n",
    "        return Tensor(stacked, func=self, name=\"vstack\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'VStack: x1: {self.x1.shape} x2: {self.x2.shape} grad: {grad.shape}')\n",
    "        grad = grad.reshape((self.x1.shape[0] + self.x2.shape[0], self.x1.shape[1], -1))\n",
    "        self.x1.backward(grad[:self.x1.shape[0], :])\n",
    "        self.x2.backward(grad[self.x1.shape[0]:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class VStack(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return GetVStack(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class GetRow(Function):\n",
    "    def __init__(self, x: Tensor, row_idx: int):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.row_idx = row_idx\n",
    "\n",
    "    def __call__(self):\n",
    "        row = self.x.data[self.row_idx]\n",
    "        return Tensor(row, func=self, name=\"row_\"+str(self.row_idx))\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Row: x: {self.x.shape} grad: {grad.shape}')\n",
    "        dx = np.zeros_like(self.x.data)\n",
    "        dx[self.row_idx] = 1\n",
    "        dx *= grad\n",
    "        self.x.backward(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Row(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, idx: int):\n",
    "        return GetRow(x, idx)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_numpy(x):\n",
    "    a = np.amax(x, axis=1)[:, np.newaxis]\n",
    "    ex = np.exp(x - a)\n",
    "    ex_sum = np.sum(ex, axis=1)[:, np.newaxis]\n",
    "    out = ex / ex_sum\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x: Tensor):\n",
    "    out = softmax_numpy(x.data)\n",
    "    return Tensor(out, func=x.func, name=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class SoftMax(Function):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __call__(self):\n",
    "        self.a = softmax_numpy(self.x.data)\n",
    "        return Tensor(self.a, func=self, name=\"softmax\")\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        # print(f'Softmax: x: {self.x.shape} grad: {grad.shape}')\n",
    "        a = self.a.reshape(-1, 1)\n",
    "        grad = np.diagflat(a) - np.dot(a, a.T)\n",
    "        self.x.backward(grad.reshape(self.x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class SoftMaxFunction(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return SoftMax(x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(inputs: Tensor, vocab_size: int):\n",
    "    seq_len, batch_size = inputs.shape\n",
    "    encoded = np.zeros((seq_len * batch_size, vocab_size))\n",
    "    encoded[np.arange(seq_len * batch_size), inputs.data.ravel().astype(int)] = 1\n",
    "    return Tensor(encoded.reshape(seq_len, batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@forks_aware\n",
    "class NLL(Function):\n",
    "    def __init__(self, y_hat: Tensor, y: Tensor, eps: float = 1e-15):\n",
    "        super().__init__()\n",
    "        self.seq_len, self.batch_size = y.shape[0], y.shape[-1]\n",
    "        num_classes = y_hat.shape[-1]\n",
    "        self.y_hat = softmax(y_hat)\n",
    "        self.y = one_hot_encoder(y, num_classes)\n",
    "        self.y = self.y.reshape(-1, num_classes)\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self):\n",
    "        logs = np.log(self.y_hat.data + self.eps)\n",
    "        loss = np.multiply(-self.y.data, logs).sum(axis=1).mean()\n",
    "        return Tensor(loss, func=self, name=\"nll\")\n",
    "    \n",
    "    def backward(self):\n",
    "        grad = self.y_hat.data - self.y.data\n",
    "        self.y_hat.backward(grad / float(self.batch_size)/ float(self.seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self, eps=1e-15):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return NLL(output, target, self.eps)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class RNNCell(Module):\n",
    "    def __init__(self, state_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = LinearLayer(state_size, hidden_size)\n",
    "        self.tanh = TanhFunction()\n",
    "        self.hstack = HStack()\n",
    "        self.register_parameters([self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Optional[Tensor] = None):\n",
    "        X = self.hstack(x, h_t_1)\n",
    "        z = self.linear(X)\n",
    "        h_t = self.tanh(z)\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = input_size + hidden_size\n",
    "        self.rnn = RNNCell(self.state_size, hidden_size)\n",
    "        self. row = Row()\n",
    "        self.vstack = VStack()\n",
    "        self.register_parameters([self.rnn])\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Optional[Tensor] = None):\n",
    "        seq_len, batch_size, input_size = x.shape\n",
    "        # print(f'seq_len: {seq_len} batch_size: {batch_size} input_size: {input_size}')\n",
    "        h = Tensor(np.zeros((0, batch_size, self.hidden_size)), name=\"h\")\n",
    "        if h_t_1 is None:\n",
    "            h_t_1 = Tensor(np.zeros((batch_size, self.hidden_size)), name=\"h_t_1\")\n",
    "        for idx in range(seq_len):\n",
    "            h_t_1 = self.rnn.forward(self.row(x, idx), h_t_1)\n",
    "            h = self.vstack(h, h_t_1.reshape((1, batch_size, self.hidden_size)))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class RecurrentNetwork(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(vocab_size, emb_size)\n",
    "        self.rnn = RNN(emb_size, hidden_size)\n",
    "        self.linear = LinearLayer(hidden_size, vocab_size)\n",
    "        xavier_(self.linear.parameters)\n",
    "        self.register_parameters([self.embedding, self.rnn, self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        emb = self.embedding(x)\n",
    "        rnn_out = self.rnn(emb)\n",
    "        linear_out = self.linear(rnn_out.reshape(-1, self.hidden_size))\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.001):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.001):\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    def __init__(self, optimizer: Optimizer, last_epoch: int = -1):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.last_epoch = last_epoch\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class ConstantLR(Scheduler):\n",
    "    def __init__(self, optimizer: Optimizer):\n",
    "        super().__init__(optimizer)\n",
    "        self.lr = optimizer.lr\n",
    "    \n",
    "    def step(self):\n",
    "        self.last_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class CosineAnnealingLR(Scheduler):\n",
    "    def __init__(self, optimizer: Optimizer, T_max: int, eta_min: float = 0, anneal_epochs: int = None, last_epoch: int = -1):\n",
    "        super().__init__(optimizer)\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.lr = optimizer.lr\n",
    "        self.last_epoch = last_epoch\n",
    "        self.start_epoch = last_epoch\n",
    "        self.anneal_epochs = anneal_epochs\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cosine_anneal(t):\n",
    "        return (1 + np.cos(np.pi * t)) / 2\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.anneal_epochs is not None:\n",
    "            passed_epochs = self.last_epoch - self.start_epoch\n",
    "            if passed_epochs > self.anneal_epochs:\n",
    "                return self.lr\n",
    "        t = self.last_epoch / self.T_max\n",
    "        return self.eta_min + (self.base_lr - self.eta_min) * self._cosine_anneal(t)\n",
    "    \n",
    "    def step(self):\n",
    "        self.lr = self.get_lr()\n",
    "        self.optimizer.lr = self.lr\n",
    "        self.last_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data, target, batch_size=20):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def next(self):\n",
    "        m, _ = self.data.shape\n",
    "        rand_index = np.random.choice(m, size=m, replace=False)\n",
    "        X, y = self.data[rand_index], self.target[rand_index]\n",
    "        pos = 0\n",
    "        while pos < m:\n",
    "            X_batch, y_batch = X[pos:pos+self.batch_size], y[pos:pos+self.batch_size]\n",
    "            yield Tensor(X_batch, name=\"x\"), Tensor(y_batch, name=\"y\")\n",
    "            pos += self.batch_size\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def eval_accuracy(model, val, y_val):\n",
    "    model.eval()\n",
    "    output = model(Tensor(val))\n",
    "    y_hat = np.argmax(output.data, axis=1)\n",
    "    model.train()\n",
    "    return accuracy_score(y_val.ravel(), y_hat.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "batch_size = 100\n",
    "dataloader = DataLoader(inputs, targets, batch_size=batch_size)\n",
    "model = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters, lr=1.0)\n",
    "# optimizer = Adam(model.parameters, alpha=0.1, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01)\n",
    "scheduler = ConstantLR(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2332"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: [50/50], loss: 3.333463983580581, acc: 0.887375"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "lrs = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_sum = 0\n",
    "    for data in dataloader():\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        targets = targets.transpose(1, 0)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.data\n",
    "    acc = eval_accuracy(model, inputs.data, targets.data)\n",
    "    print(f'\\r epoch: [{epoch+1}/{num_epochs}], loss: {loss_sum}, acc: {acc}', end='')\n",
    "    losses.append(loss_sum)\n",
    "    accuracies.append(acc)\n",
    "    lrs.append(scheduler.lr)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(num_epochs)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(epochs, losses)\n",
    "ax[0].set_title('Losses')\n",
    "ax[1].plot(epochs, accuracies)\n",
    "ax[1].set_title('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 100\n",
    "seq_len = 3\n",
    "max_number = 10\n",
    "\n",
    "inputs, targets = get_examples(seq_len, num_examples, max_number)\n",
    "\n",
    "inputs = to_string(inputs, seq_len, max_number)\n",
    "targets = to_string(targets, seq_len, max_number)\n",
    "\n",
    "inputs = integer_encode(inputs, vocab)\n",
    "targets = integer_encode(targets, vocab)\n",
    "\n",
    "inputs, targets = Tensor(np.array(inputs).transpose((1, 0))), Tensor(np.array(targets).transpose((1, 0)))\n",
    "outputs = model(inputs)\n",
    "predicted = np.argmax(outputs.data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# gg_confusion_matrix(targets.data.reshape(-1), predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checker(J, grad_J, theta, eps=1e-5, rtol=1e-5):\n",
    "    \"\"\"Gradient checker for scalar and vector functions\n",
    "Args:\n",
    "    J - function of theta\n",
    "    grad_J - gradient of function J\n",
    "    theta - the point for which to compute the numerical gradient\n",
    "    eps - step value in numerical gradient\n",
    "    rtol - relative tolerance threshold value\n",
    "Returns:\n",
    "    error message if the relative tolerance is greater for some axis\n",
    "    or \"Gradient check passed\" else\n",
    "\"\"\"\n",
    "    it = np.nditer(theta, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] += eps\n",
    "        J1 = J(theta_)\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] -= eps\n",
    "        J2 = J(theta_)\n",
    "\n",
    "        J1 = to_scalar(J1, ix)\n",
    "        J2 = to_scalar(J2, ix)\n",
    "\n",
    "        num_grad = (J1 - J2)/(2*eps)\n",
    "\n",
    "        rel_tol = np.abs(num_grad - grad_J)[ix]/(1. + np.minimum(np.abs(num_grad), np.abs(grad_J[ix])))\n",
    "\n",
    "        if rel_tol > rtol:\n",
    "            print(f'num_grad: {num_grad} grad: {grad_J[ix]} factor: {grad_J[ix] / num_grad}')\n",
    "            print(f'Incorrect gradient for the axis {str(ix)}')\n",
    "            return\n",
    "        it.iternext()\n",
    "    print(f'Gradient check passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def J_theta_global(model, loss_function, theta, idx, x, y):\n",
    "    original = model.parameters[idx].data.copy()\n",
    "    np.copyto(dst=model.parameters[idx].data, src=theta)\n",
    "    outputs = model(x)\n",
    "    loss_value = loss_function(outputs, y).data\n",
    "    np.copyto(dst=model.parameters[idx].data, src=original)\n",
    "    model.zero_grad()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def dJ_theta_global(model, loss_function, x, y):\n",
    "    outputs = model(x)\n",
    "    loss = loss_function(outputs, y)\n",
    "    loss.backward()\n",
    "    grads = []\n",
    "    for parameter in model.parameters:\n",
    "        grads.append(parameter.grad.copy())\n",
    "    model.zero_grad()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 100\n",
    "seq_len = 2\n",
    "max_number = 10\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "\n",
    "X_val, y_val = get_examples(seq_len, num_examples, max_number)\n",
    "X_val, y_val = X_val.transpose(1, 0), y_val.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: Start -- E\n",
      "Gradient check passed\n",
      "[0]: Elapsed time: 0.1s\n",
      "[1]: Start -- weights\n",
      "Gradient check passed\n",
      "[1]: Elapsed time: 1.0s\n",
      "[2]: Start -- bias\n",
      "Gradient check passed\n",
      "[2]: Elapsed time: 0.0s\n",
      "[3]: Start -- weights\n",
      "Gradient check passed\n",
      "[3]: Elapsed time: 0.2s\n",
      "[4]: Start -- bias\n",
      "Gradient check passed\n",
      "[4]: Elapsed time: 0.0s\n",
      "Total elapsed time: 1.4s\n"
     ]
    }
   ],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "model_ = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "dJ_theta_tensors = dJ_theta_global(model_, loss_function, Tensor(X_val), Tensor(y_val))\n",
    "global_start = time.time()\n",
    "for i, parameter in enumerate(model_.parameters):\n",
    "    start = time.time()\n",
    "    print(f'[{i}]: Start -- {parameter.__name__}')\n",
    "    def J_theta(theta, idx=i, x=Tensor(X_val), y=Tensor(y_val)):\n",
    "        return J_theta_global(model_, loss_function, theta, idx, x, y)\n",
    "    gradient_checker(J_theta, dJ_theta_tensors[i], parameter.data)\n",
    "    print(f'[{i}]: Elapsed time: {time.time() - start:.1f}s')\n",
    "print(f'Total elapsed time: {time.time() - global_start:.1f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# 1. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Problem 1 (6.0)\n",
    "\n",
    "Implement the LSTM cell class and use it in the RNN sorter. Don't forget to check gradients!\n",
    "\n",
    "LSTM cell formulas:\n",
    "$$\n",
    "f_t=\\sigma\\Bigg(W_f\\cdot\\Big[h_{t-1};x_t\\Big] + b_f\\Bigg)\\\\\n",
    "i_t = \\sigma\\Bigg(W_i\\cdot\\Big[h_{t-1};x_t\\Big] + b_i\\Bigg)\\\\\n",
    "\\tilde{c}_t = \\tanh\\Bigg(W_c\\cdot\\Big[h_{t-1};x_t\\Big] + b_c\\Bigg)\\\\\n",
    "c_t = f_t \\ast c_{t-1} + i_t \\ast \\tilde{c}_t\\\\\n",
    "o_t = \\sigma\\Bigg(W_o\\cdot\\Big[h_{t-1};x_t\\Big] + b_o\\Bigg)\\\\\n",
    "h_t = o_t \\ast \\tanh \\left(c_t\\right)\n",
    "$$\n",
    "\n",
    "As you can see, you'll need two additional functions here: sum and multiplication. You have to implement them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class MultiplyFunction(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        super().__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        return Tensor(np.multiply(self.x1.data, self.x2.data), func=self)\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        self.x1.backward(np.multiply(grad, self.x2.data).reshape(self.x1.shape))\n",
    "        self.x2.backward(np.multiply(grad, self.x1.data).reshape(self.x2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Multiply(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return MultiplyFunction(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class SumFunction(Function):\n",
    "    def __init__(self, x1: Tensor, x2: Tensor):\n",
    "        super().__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __call__(self):\n",
    "        return Tensor(self.x1.data + self.x2.data, func=self)\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        self.x1.backward(grad.reshape(self.x1.shape))\n",
    "        self.x2.backward(grad.reshape(self.x2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Sum(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor):\n",
    "        return SumFunction(x1, x2)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "\n",
    "    def __init__(self, state_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.layer_f = LinearLayer(self.state_size, self.hidden_size)\n",
    "        self.layer_i = LinearLayer(self.state_size, self.hidden_size)\n",
    "        self.layer_c = LinearLayer(self.state_size, self.hidden_size)\n",
    "        self.layer_o = LinearLayer(self.state_size, self.hidden_size)\n",
    "\n",
    "        self.register_parameters([self.layer_f, self.layer_i, self.layer_c, self.layer_o])\n",
    "\n",
    "        self.function_f = SigmoidFunction()\n",
    "        self.function_i = SigmoidFunction()\n",
    "        self.function_c = TanhFunction()\n",
    "        self.function_o = SigmoidFunction()\n",
    "        self.function_h = TanhFunction()\n",
    "\n",
    "        self.multiply_c1 = Multiply()\n",
    "        self.multiply_c2 = Multiply()\n",
    "        self.multiply_h = Multiply()\n",
    "\n",
    "        self.sum = Sum()\n",
    "\n",
    "        self.hstack = HStack()\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Tensor, c_t_1: Tensor):\n",
    "        X = self.hstack(h_t_1, x)\n",
    "\n",
    "        f_result = self.function_f(self.layer_f(X))\n",
    "        i_result = self.function_i(self.layer_i(X))\n",
    "        c_result = self.function_c(self.layer_c(X))\n",
    "        o_result = self.function_o(self.layer_o(X))\n",
    "\n",
    "        result_c = self.sum(self.multiply_c1(f_result, c_t_1), self.multiply_c2(i_result, c_result))\n",
    "        result_h = self.multiply_h(o_result, self.function_h(result_c))\n",
    "\n",
    "        return result_h, result_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm_cell = LSTMCell(self.input_size + self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.register_parameters([self.lstm_cell])\n",
    "\n",
    "        self.vstack = VStack()\n",
    "        self.row = Row()\n",
    "\n",
    "    def forward(self, x: Tensor, h_t_1: Optional[Tensor] = None, c_t_1: Optional[Tensor] = None):\n",
    "        batch_size = x.shape[1]\n",
    "        h = Tensor (np.zeros((0, batch_size, self.hidden_size)), name=\"h\")\n",
    "\n",
    "        h_t_1 = h_t_1 if h_t_1 is not None else Tensor(np.zeros((batch_size, self.hidden_size)), name=\"h_t_1\")\n",
    "        c_t_1 = c_t_1 if c_t_1 is not None else Tensor(np.zeros((batch_size, self.hidden_size)), name=\"c_t_1\")\n",
    "\n",
    "        for idx in range(seq_len):\n",
    "            h_t_1, c_t_1 = self.lstm_cell.forward(self.row(x, idx), h_t_1, c_t_1)\n",
    "            h = self.vstack(h, h_t_1.reshape((1, batch_size, self.hidden_size)))\n",
    "\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params: List[Tensor], alpha=0.1, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
    "        super().__init__(params, alpha)\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.t = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Checks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "# Checks\n",
    "Change RNN to LSTM in the RecurrentNetwork class below to perform gradient check.\n",
    "Note, that due to naive implementation of the backpropagation, number of calculations needed for one backpropagation pass grows exponentially with respect to sequence length. That's why the length in the gradient check is set to 2.\n",
    "\n",
    "$\\color{red}{\\text{If you want to implement a faster version of numpy framework, which supports recurrent arcgitectures, you can take this task as a course project.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class RecurrentNetwork(Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(vocab_size, emb_size)\n",
    "        self.lstm = LSTM(emb_size, hidden_size)\n",
    "        self.linear = LinearLayer(hidden_size, vocab_size)\n",
    "        xavier_(self.linear.parameters)\n",
    "        self.register_parameters([self.embedding, self.lstm, self.linear])\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        emb = self.embedding(x)\n",
    "        lstm_out = self.lstm(emb)\n",
    "        linear_out = self.linear(lstm_out.reshape(-1, self.hidden_size))\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checker(J, grad_J, theta, eps=1e-5, rtol=1e-5):\n",
    "    \"\"\"Gradient checker for scalar and vector functions\n",
    "Args:\n",
    "    J - function of theta\n",
    "    grad_J - gradient of function J\n",
    "    theta - the point for which to compute the numerical gradient\n",
    "    eps - step value in numerical gradient\n",
    "    rtol - relative tolerance threshold value\n",
    "Returns:\n",
    "    error message if the relative tolerance is greater for some axis\n",
    "    or \"Gradient check passed\" else\n",
    "\"\"\"\n",
    "    it = np.nditer(theta, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    calls = 0 \n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] += eps\n",
    "        J1 = J(theta_)\n",
    "        theta_ = np.array(theta, copy=True)\n",
    "        theta_[ix] -= eps\n",
    "        J2 = J(theta_)\n",
    "        calls += 1\n",
    "        J1 = to_scalar(J1, ix)\n",
    "        J2 = to_scalar(J2, ix)\n",
    "\n",
    "        num_grad = (J1 - J2)/(2*eps)\n",
    "\n",
    "        rel_tol = np.abs(num_grad - grad_J)[ix]/(1. + np.minimum(np.abs(num_grad), np.abs(grad_J[ix])))\n",
    "\n",
    "        if rel_tol > rtol:\n",
    "            print(f'num_grad: {num_grad} grad: {grad_J[ix]} factor: {grad_J[ix] / num_grad}')\n",
    "            print(f'Incorrect gradient for the axis {str(ix)}')\n",
    "            print(f'calls: {calls}')\n",
    "            return\n",
    "        it.iternext()\n",
    "    print(f'Gradient check passed. Calls: {calls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def J_theta_global(model, loss_function, theta, idx, x, y):\n",
    "    original = model.parameters[idx].data.copy()\n",
    "    np.copyto(dst=model.parameters[idx].data, src=theta)\n",
    "    outputs = model(x)\n",
    "    loss_value = loss_function(outputs, y).data\n",
    "    np.copyto(dst=model.parameters[idx].data, src=original)\n",
    "    model.zero_grad()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def dJ_theta_global(model, loss_function, x, y):\n",
    "    outputs = model(x)\n",
    "    loss = loss_function(outputs, y)\n",
    "    loss.backward()\n",
    "    grads = []\n",
    "    for parameter in model.parameters:\n",
    "        grads.append(parameter.grad.copy())\n",
    "    model.zero_grad()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# original - passes test\n",
    "num_examples = 100\n",
    "seq_len = 2\n",
    "max_number = 10\n",
    "vocab_size = len(vocab)\n",
    "emb_size = 20\n",
    "hidden_size = 32\n",
    "\n",
    "X_val, y_val = get_examples(seq_len, num_examples, max_number)\n",
    "X_val, y_val = X_val.transpose(1, 0), y_val.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: Start -- E\n",
      "Gradient check passed. Calls: 240\n",
      "[0]: Elapsed time: 0.3s\n",
      "[1]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[1]: Elapsed time: 2.3s\n",
      "[2]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[2]: Elapsed time: 0.0s\n",
      "[3]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[3]: Elapsed time: 3.1s\n",
      "[4]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[4]: Elapsed time: 0.0s\n",
      "[5]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[5]: Elapsed time: 2.3s\n",
      "[6]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[6]: Elapsed time: 0.0s\n",
      "[7]: Start -- weights\n",
      "Gradient check passed. Calls: 1664\n",
      "[7]: Elapsed time: 2.4s\n",
      "[8]: Start -- bias\n",
      "Gradient check passed. Calls: 32\n",
      "[8]: Elapsed time: 0.0s\n",
      "[9]: Start -- weights\n",
      "Gradient check passed. Calls: 384\n",
      "[9]: Elapsed time: 0.5s\n",
      "[10]: Start -- bias\n",
      "Gradient check passed. Calls: 12\n",
      "[10]: Elapsed time: 0.0s\n",
      "Total elapsed time: 11.2s\n"
     ]
    }
   ],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "model_ = RecurrentNetwork(vocab_size, emb_size, hidden_size)\n",
    "dJ_theta_tensors = dJ_theta_global(model_, loss_function, Tensor(X_val), Tensor(y_val))\n",
    "global_start = time.time()\n",
    "for i, parameter in enumerate(model_.parameters):\n",
    "    start = time.time()\n",
    "    print(f'[{i}]: Start -- {parameter.__name__}')\n",
    "    def J_theta(theta, idx=i, x=Tensor(X_val), y=Tensor(y_val)):\n",
    "        return J_theta_global(model_, loss_function, theta, idx, x, y)\n",
    "    gradient_checker(J_theta, dJ_theta_tensors[i], parameter.data)\n",
    "    print(f'[{i}]: Elapsed time: {time.time() - start:.1f}s')\n",
    "print(f'Total elapsed time: {time.time() - global_start:.1f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# 2. Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "# Problem 2 (6.0) Model Distillation\n",
    "\n",
    "    Model Distillation   DistillBert  HuggingFace.                -         -.\n",
    "\n",
    "  [](https://habr.com/ru/company/avito/blog/485290/)       - .    BERT,   BiLSTM  - ,      - .\n",
    "\n",
    "  GitHub  Colab c     . "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# DUE DATE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "# The due date is 22 of December 2021 23:59:59\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* To submit the assignment share your workbook to me (with **Can edit** access rights).\n",
    "\n",
    "* Check, that you found all the tasks in the workbook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "computation_mode": "JUPYTER",
   "packages": [],
   "version": 1
  },
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
