{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit dictionaries downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/simon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/simon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/simon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/simon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def load_map(file_name):\n",
    "    result = pandas.read_csv(f'data/{file_name}.csv')\n",
    "    return { str(row[\"key\"]): str(row[\"value\"]) for i, row in result.iterrows() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_dict = {\n",
    "    'URL': r\"\"\"(?xi)\\b(?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.|pic\\.|twitter\\.|facebook\\.)(?:\\([-A-Z0-9+&@#\\/%=~_|$?!:;,.]*\\)|[-A-Z0-9+&@#\\/%=~_|$?!:;,.])*(?:\\([-A-Z0-9+&@#\\/%=~_|$?!:,.]*\\)|[A-Z0-9+&@#\\/%=~_|$])\"\"\",\n",
    "    'EMOJI': u'([\\U0001F1E0-\\U0001F1FF])|([\\U0001F300-\\U0001F5FF])|([\\U0001F600-\\U0001F64F])|([\\U0001F680-\\U0001F6FF])|([\\U0001F700-\\U0001F77F])|([\\U0001F800-\\U0001F8FF])|([\\U0001F900-\\U0001F9FF])|([\\U0001FA00-\\U0001FA6F])|([\\U0001FA70-\\U0001FAFF])|([\\U00002702-\\U000027B0])|([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])',\n",
    "    'HASHTAG': r\"\\#\\b[\\w\\-\\_]+\\b\",\n",
    "    'EMAIL': r\"(?:^|(?<=[^\\w@.)]))(?:[\\w+-](?:\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(?:\\.(?:[a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
    "    'MENTION': r\"@[A-Za-z0-9]+\",\n",
    "    'CASHTAG': r\"(?:[$\\u20ac\\u00a3\\u00a2]\\d+(?:[\\\\.,']\\d+)?(?:[MmKkBb](?:n|(?:il(?:lion)?))?)?)|(?:\\d+(?:[\\\\.,']\\\\d+)?[$\\u20ac\\u00a3\\u00a2])\",\n",
    "    'DATE': r\"(?:(?:(?:(?:(?<!:)\\b\\'?\\d{1,4},? ?)?\\b(?:[Jj]an(?:uary)?|[Ff]eb(?:ruary)?|[Mm]ar(?:ch)?|[Aa]pr(?:il)?|May|[Jj]un(?:e)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ept?(?:ember)?|[Oo]ct(?:ober)?|[Nn]ov(?:ember)?|[Dd]ec(?:ember)?)\\b(?:(?:,? ?\\'?)?\\d{1,4}(?:st|nd|rd|n?th)?\\b(?:[,\\\\/]? ?\\'?\\d{2,4}[a-zA-Z]*)?(?: ?- ?\\d{2,4}[a-zA-Z]*)?(?!:\\d{1,4})\\b))|(?:(?:(?<!:)\\b\\\\'?\\d{1,4},? ?)\\b(?:[Jj]an(?:uary)?|[Ff]eb(?:ruary)?|[Mm]ar(?:ch)?|[Aa]pr(?:il)?|May|[Jj]un(?:e)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ept?(?:ember)?|[Oo]ct(?:ober)?|[Nn]ov(?:ember)?|[Dd]ec(?:ember)?)\\b(?:(?:,? ?\\'?)?\\d{1,4}(?:st|nd|rd|n?th)?\\b(?:[,\\\\/]? ?\\'?\\d{2,4}[a-zA-Z]*)?(?: ?- ?\\d{2,4}[a-zA-Z]*)?(?!:\\d{1,4})\\b)?))|(?:\\b(?<!\\d\\\\.)(?:(?:(?:[0123]?[0-9][\\\\.\\\\-\\\\/])?[0123]?[0-9][\\\\.\\\\-\\\\/][12][0-9]{3})|(?:[0123]?[0-9][\\\\.\\\\-\\\\/][0123]?[0-9][\\\\.\\\\-\\\\/][12]?[0-9]{2,3}))(?!\\.\\d)\\b))\",\n",
    "    'TIME': r'(?:(?:\\d+)?\\\\.?\\d+(?:AM|PM|am|pm|a\\\\.m\\\\.|p\\\\.m\\\\.))|(?:(?:[0-2]?[0-9]|[2][0-3]):(?:[0-5][0-9])(?::(?:[0-5][0-9]))?(?: ?(?:AM|PM|am|pm|a\\\\.m\\\\.|p\\\\.m\\\\.))?)',\n",
    "    'EMPHASIS': r\"(?:\\*\\b\\w+\\b\\*)\",\n",
    "    'ELONG': r\"\\b[A-Za-z]*([a-zA-Z])\\1\\1[A-Za-z]*\\b\",\n",
    "    'PUNCTUATION': r\"[\\-\\\"`@#$%^&*(|)/~\\[\\]{\\}!:;+,._='?]+\"\n",
    "}\n",
    "\n",
    "regex = { k: re.compile(regex_dict[k]) for k, v in regex_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "emnlp = load_map(\"emnlp_dict\")\n",
    "contraction_mapping = load_map(\"contraction_mapping\")\n",
    "emoticons = load_map(\"emoticons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=2)\n",
    "char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_string(string):\n",
    "    string = string.lower()\n",
    "\n",
    "    for word, word_replace in contraction_mapping.items():\n",
    "        string = string.replace(word, word_replace)\n",
    "\n",
    "    for key, reg in regex.items():\n",
    "        string = reg.sub(f\" {key} \", string)\n",
    "\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', errors='ignore').decode('utf8', errors='ignore')\n",
    "    string = word_tokenize(string)\n",
    "    string = [emnlp[word] if str(word) in emnlp else word for word in string]\n",
    "    string = [lemmatizer.lemmatize(word) for word in string if not word in stop_words]\n",
    "    string = ' '.join(string)\n",
    "    return string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organization prediction\n",
    "\n",
    "- The ability to predict organization (e.g. Apple) given a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "company_vectorizer = Pipeline([\n",
    "    ('feats', FeatureUnion(\n",
    "        [\n",
    "            ('word_ngram', word_vectorizer),\n",
    "            ('char_ngram', char_vectorizer)\n",
    "        ]\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def normalize_string_for_company(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r'(@|#)(apple|microsoft|windows|xbox|office|google|twitter|android)', r'\\2', string)\n",
    "\n",
    "    for word in emoticons.keys():\n",
    "        string = string.replace(word, \" \")\n",
    "\n",
    "    return normalize_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pandas.read_csv(\"data/test.csv\")\n",
    "train_df = pandas.read_csv(\"data/train.csv\")\n",
    "\n",
    "train_cleaned = train_df['TweetText'].apply(normalize_string_for_company)\n",
    "test_cleaned = test_df['TweetText'].apply(normalize_string_for_company)\n",
    "\n",
    "company_vectorizer = company_vectorizer.fit(train_cleaned)\n",
    "\n",
    "x_train = company_vectorizer.transform(train_cleaned)\n",
    "x_test = company_vectorizer.transform(test_cleaned)\n",
    "\n",
    "y_train = train_df['Topic']\n",
    "y_test = test_df['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       0.95      0.96      0.95        98\n",
      "      google       0.85      0.80      0.82        79\n",
      "   microsoft       0.81      0.73      0.77        78\n",
      "     twitter       0.75      0.85      0.80        87\n",
      "\n",
      "    accuracy                           0.84       342\n",
      "   macro avg       0.84      0.83      0.84       342\n",
      "weighted avg       0.84      0.84      0.84       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(x_train,y_train)\n",
    "\n",
    "pred_company = svc.predict(x_test)\n",
    "print(classification_report(y_test,pred_company))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vectorizer = Pipeline([\n",
    "    ('feats', FeatureUnion(\n",
    "        [\n",
    "            ('word_ngram', word_vectorizer),\n",
    "            ('char_ngram', char_vectorizer)\n",
    "        ]\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def normalize_string_for_sentiment(string):\n",
    "    for word, word_replace in emoticons.items():\n",
    "        string = string.replace(word, word_replace[1:-1])\n",
    "\n",
    "    return normalize_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import numpy as np\n",
    "\n",
    "def transform_topic(v, name, value=0.1):\n",
    "    return [value if x == name else 0 for x in v]\n",
    "\n",
    "test_df = pandas.read_csv(\"data/test.csv\")\n",
    "test_df = test_df[test_df[\"Sentiment\"] != \"irrelevant\"]\n",
    "\n",
    "train_df = pandas.read_csv(\"data/train.csv\")\n",
    "train_df = train_df[train_df[\"Sentiment\"] != \"irrelevant\"]\n",
    "\n",
    "train_cleaned = train_df['TweetText'].apply(normalize_string_for_company)\n",
    "test_cleaned = test_df['TweetText'].apply(normalize_string_for_company)\n",
    "\n",
    "sentiment_vectorizer = sentiment_vectorizer.fit(train_cleaned)\n",
    "\n",
    "x_train = sentiment_vectorizer.transform(train_cleaned)\n",
    "x_train = scipy.sparse.hstack((\n",
    "    x_train, \n",
    "    np.asmatrix(transform_topic(train_df['Topic'].values, \"positive\")).transpose(),\n",
    "    np.asmatrix(transform_topic(train_df['Topic'].values, \"neutral\")).transpose(),\n",
    "    np.asmatrix(transform_topic(train_df['Topic'].values, \"negative\")).transpose()\n",
    "))\n",
    "\n",
    "x_test = sentiment_vectorizer.transform(test_cleaned)\n",
    "x_test = scipy.sparse.hstack((\n",
    "    x_test, \n",
    "    np.asmatrix(transform_topic(test_df['Topic'].values, \"positive\")).transpose(),\n",
    "    np.asmatrix(transform_topic(test_df['Topic'].values, \"neutral\")).transpose(),\n",
    "    np.asmatrix(transform_topic(test_df['Topic'].values, \"negative\")).transpose()\n",
    "))\n",
    "\n",
    "y_train_named = train_df['Sentiment']\n",
    "y_test = test_df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expand to more than 3 sentiment classes (say in 5-point scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.63      0.55        38\n",
      "     neutral       0.88      0.80      0.84       173\n",
      "    positive       0.53      0.65      0.59        26\n",
      "\n",
      "    accuracy                           0.76       237\n",
      "   macro avg       0.64      0.69      0.66       237\n",
      "weighted avg       0.78      0.76      0.77       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR, LinearSVC\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def divide_into_groups(x):\n",
    "    negative = x[0]\n",
    "    neutral = x[1]\n",
    "    positive = x[2]\n",
    "    \n",
    "    if neutral >= negative and neutral >= positive:\n",
    "        if neutral - positive < 0.3:\n",
    "            return \"semi-positive\"\n",
    "        if neutral - negative < 0.3:\n",
    "            return \"semi-negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    if negative >= neutral and negative >= positive:\n",
    "        if negative < 0:\n",
    "            return \"semi-negative\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "    if positive >= negative and positive >= neutral:\n",
    "        if positive < 0:\n",
    "            return \"semi-positive\"\n",
    "        else:\n",
    "            return \"positive\"\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(x_train,y_train_named)\n",
    "pred_svc = svc.predict(x_test)\n",
    "\n",
    "print(classification_report(pred_svc, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fafece7a86de58194924b2ee1cdae1ab79ef7c958fbe270ad66b954dab7cc277"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
